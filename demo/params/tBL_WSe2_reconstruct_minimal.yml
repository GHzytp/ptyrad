# Created with PtyRAD 0.1.0b8
# Requires PtyRAD >= 0.1.0b8
# Latest demo params files / scripts: https://github.com/chiahao3/ptyrad/tree/main/demo
# Documentation: https://ptyrad.readthedocs.io/en/latest/

# tBL_WSe2

init_params : {
    # Experimental params
    'probe_kv'               : 80, # type: float, unit: kV. Acceleration voltage for relativistic electron wavelength calculation
    'probe_conv_angle'       : 24.9, # type: float, unit: mrad. Semi-convergence angle for probe-forming aperture
    'probe_defocus'          : 0, # type: float, unit: Ang. Defocus (-C1) aberration coefficient for the probe. Positive defocus here refers to actual underfocus or weaker lens strength following Kirkland/abtem/ptychoshelves convention
    'meas_Npix'              : 128, # type: integer, unit: px (k-space). Detector pixel number, EMPAD is 128. Only supports square detector for simplicity
    'pos_N_scan_slow'        : 128, # type: int, unit: #. Number of scan position along slow scan direction. Usually it's the vertical direction of acquisition GUI
    'pos_N_scan_fast'        : 128, # type: int, unit: #. Number of scan position along fast scan direction. Usually it's the horizontal direction of acquisition GUI
    'pos_scan_step_size'     : 0.4290, # type: float, unit: Ang. Step size between probe positions in a rectangular raster scan pattern
    # Model complexity
    'probe_pmode_max'        : 6, # type: int, unit: #. Maximum number of mixed probe modes. Set to pmode_max = 1 for single probe state, pmode_max > 1 for mixed-state probe during initialization. For simulated initial probe, it'll be generated with the specified number of probe modes. For loaded probe, the pmode dimension would be capped at this number
    'obj_Nlayer'             : 6, # type: int, unit: #. Number of slices for multislice object
    'obj_slice_thickness'    : 2, # type: float, unit: Ang. Slice thickness (propagation distance) for multislice ptychography. Typical values are between 1 to 20 Ang
    # Preprocessing
    'meas_flipT'             : [1,0,0], # type: null or list of 3 binary booleans (0 or 1) as [flipup, fliplr, transpose] just like PtychoShleves. Default is null or [0,0,0] but you may need to find the correct flip and transpose to match your dataset configuration. This applies additional flip and transpose to initialized diffraction patterns. It's suggested to use 'meas_flipT' to correct the dataset orientation and this is the only orientaiton-related value attached to output reconstruction folder name
    # Input source and params
    'meas_params'            : {'path': 'data/tBL_WSe2/Panel_g-h_Themis/scan_x128_y128.raw'}, # type: dict, or numpy array. For file type of 'mat', or 'hdf5', it's preferred to provide both the 'path' and 'key' in a dict {'path': <PATH_TO_DATA>, 'key': <DATA_KEY>} to retrieve the data matrix, although PtyRAD would try to retrive the data even without a key. 'tif' would only need the {'path':<PATH_TO_DATA>}. For 'raw' you can optionally pass in 'shape':(N,height,width), 'offset':int, 'gap':int to load the .raw files from EMPAD1 and pre-processed EMPAD datasets. For example, {'path': <PATH_TO_DATA>, 'offset':0, 'gap':0} can be used to load pre-processed EMPAD2 raw dataset with no gap between binary diffraction patterns. The 'shape' will be automatically filled in from 'exp_params', while 'offset':0, and 'gap':1024 are default values for EMPAD1 datasets. For py4dstem processed diffraction patterns (hdf5), use '/datacube_root/datacube/data' for your 'key'. For 'custom' source, pass the numpy array to the 'measurements_params' entry after you load this .yml as a dict
}

recon_params: {
    'NITER': 200, # type: int. Total number of reconstruction iterations. 1 iteration means a full pass of all selected diffraction patterns. Usually 20-50 iterations can get 90% of the work done with a proper learning rate between 1e-3 to 1e-4. For faster trials in hypertune mode, set 'NITER' to a smaller number than your typical reconstruction to save time. Usually 10-20 iterations are enough for the hypertune parameters to show their relative performance. 
    'SAVE_ITERS': 10, # type: null or int. Number of completed iterations before saving the current reconstruction results (model, probe, object) and summary figures. If 'SAVE_ITERS' is 50, it'll create an output reconstruction folder and save the results and figures into it every 50 iterations. If null, the output reconstruction folder would not be created and no reconstruction results or summary figures would be saved. If 'SAVE_ITERS' > 'NITER', it'll create the output reconstruction folder but no results / figs would be saved. Typically we set 'SAVE_ITERS' to 50 for reconstruction mode with 'NITER' around 200 to 500. For hypertune mode, it's suggested to set 'SAVE_ITERS' to null and set 'collate_results' to true to save the disk space, while also provide an convenient way to check the hypertune performance by the collated results.
    'BATCH_SIZE': {'size': 32, 'grad_accumulation': 1}, # type: dict. Batch size is the number of diffraction patterns processed simultaneously to get the gradient update. 'size' is the number of diffraction pattern in a sub-batch, and 'grad_accumulation' is how many sub-batches' gradients are accumulated before applying the update. Effective batch size (for 1 update) is batch_size * grad_accumulation. Gradient accumulation is a ML technique that allows people to use large effective batch size by trading the iteration time with memory requirement, so if you can fit the entire batch inside your memory, you should always set 'grad_accumulation': 1 for performance. "Batch size" is commonly used in machine learning community, while it's called "grouping" in PtychoShelves. Batch size has an effect on both convergence speed and final quality, usually smaller batch size leads to better final quality for iterative gradient descent, but smaller batch size would also lead to longer computation time per iteration because the GPU isn't as utilized as large batch sizes (due to less GPU parallelism). On the other hand, large batch size is known to be more robust (noise-resilient) but converges slower. Generally batch size of 32 to 128 is used, although certain algorithms (like ePIE) would prefer a large batch size that is equal to the dataset size for robustness. For extremely large object (or with a lot of object modes), you'll need to reduce batch size to save GPU memory, or use `grad_accumulation` to split a batch into multiple sub-batches for 1 update.
    'output_dir': 'output/tBL_WSe2/', # type str. Path and name of the main output directory. Ideally the 'output_dir' keeps a series of reconstruction of the same materials system or project. The PtyRAD results and figs will be saved into a reconstruction-specific folder under 'output_dir'. The 'output_dir' folder will be automatically created if it doesn't exist.
}

# Notes about .yml (YAML) file format
# - For each mapping, leave a space after the :, like <key>: <value> so it can be correctly parsed into Python dict
# - Use null (YAML) for None (Python)
# - Use [] for list and {} for dict, avoid () because they might not be parsed correctly
# - For commenting, leave a space before the #
# - The comment for each entry is provided with type, unit, physical meaning, and suggested usage if applicable.