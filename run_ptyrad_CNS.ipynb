{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # PtyRAD - PTYchographic Reconstruction with Automatic Differentiation\n",
    "\n",
    " Chia-Hao Lee\n",
    "\n",
    "cl2696@cornell.edu\n",
    "\n",
    "Created 2024.03.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import load_fields_from_mat, read_matv7_3\n",
    "from preprocess import preprocess_ptycho_output_dict \n",
    "from utils import complex_object_interp3d, near_field_evolution\n",
    "import optimization\n",
    "import models\n",
    "import visualization\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imwrite\n",
    "import torch\n",
    "\n",
    "GPUID = 0\n",
    "DEVICE = torch.device(\"cuda:\" + str(GPUID))\n",
    "print(\"Execution device: \", DEVICE)\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(GPUID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data path\n",
    "ptycho_output_mat_path = \"data/CNS_from_Hari/local_AD.mat\" #\"data/CNS_from_Hari/Niter10000.mat\"\n",
    "exp_CBED_path =          \"data/CNS_from_Hari/231002_fov_23p044A_x_24p402A_thickness_9p978A_step0p28_conv30_dfm100_det70_TDS_500configs_xdirection_Co_0p25_Nb_0_S_0.mat\"\n",
    "\n",
    "print(\"Loading ptycho output and input CBED\")\n",
    "ptycho_output_dict = load_fields_from_mat(ptycho_output_mat_path, 'All', squeeze_me=True, simplify_cells=True)\n",
    "input_CBED         = read_matv7_3(exp_CBED_path, 'cbed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the CBED orientation\n",
    "print(f\"input_CBED.shape = {input_CBED.shape}\") # The .mat reads (164,164,82,87), note the 87 corresponds to the object horizontal direction (noted as N_scan_y in Hari's notation)\n",
    "input_CBED_reshape = input_CBED.reshape(input_CBED.shape[0]*input_CBED.shape[1], input_CBED.shape[2], input_CBED.shape[3], order = 'C')\n",
    "print(f\"input_CBED.shape = {input_CBED_reshape.shape}\")\n",
    "input_CBED_reshape = input_CBED_reshape.transpose(0,2,1) # To swap ky into the 1st dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and setting up the data dimension\n",
    "print(\"Preprocessing ptycho output and experimental CBED\\n\")\n",
    "probe, object, exp_params = preprocess_ptycho_output_dict(ptycho_output_dict)\n",
    "cbeds                     = input_CBED_reshape # preprocess_CBED(exp_CBED) # (Nscan, ky, kx) that matches the .tif view\n",
    "\n",
    "# Prepare the experimental param for forward model and dataset generation\n",
    "lambd           = exp_params['lambd']\n",
    "dx_spec         = exp_params['dx_spec']\n",
    "z_distance      = 10 #exp_params['z_distance_arr'] # Ang, for 2D input, put the final desired total thickness if you're planning to do multislice\n",
    "probe_positions = exp_params['probe_positions']\n",
    "Nlayer          = exp_params['Nlayer']\n",
    "N_scans         = exp_params['N_scans']\n",
    "\n",
    "# Preprocessing variables for tBL-WSe2 with Themis\n",
    "object = object[None, :,:]#object.transpose((2,0,1)) # Converting object into (Nz, Ny, Nx)\n",
    "probe = probe.transpose((2,0,1))        # Converting probe into (pmode, Ny, Nx)\n",
    "      \n",
    "print(f\"\\nobject dtype/shape          (Nz, Ny, Nx) = {object.dtype}, {object.shape},\\\n",
    "        \\nprobe data dtype/shape   (pmode, Ny, Nx) = {probe.dtype}, {probe.shape},\\\n",
    "        \\ncbeds data dtype/shape       (N, Ky, Kx) = {cbeds.dtype}, {cbeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the object, probe, probe position, propagator in np array\n",
    "\n",
    "# Reslice the z slices\n",
    "final_z = 10 # z slices #21 for PSO\n",
    "z_zoom = final_z / Nlayer\n",
    "z_distance = z_distance / z_zoom # Scale the interlayer distance based on z_zoom\n",
    "z_extent = final_z * z_distance\n",
    "object = complex_object_interp3d(object, (z_zoom, 1, 1), z_axis = 0, use_np_or_cp = 'np') # Use cp for faster interpolation and convert it back to np with .get() as a default postprocessing\n",
    "print(f\"z_zoom = {z_zoom}, final_z = {final_z} slices, z_distance = {z_distance:.4f} Ang, z_extent = {z_extent:4f} Ang\")\n",
    "\n",
    "# Calculate the crop coordinates with floating points\n",
    "probe_positions = probe_positions[:, [1,0]] # The first index after shifting is the row index (along vertical axis)\n",
    "crop_coordinates = probe_positions + np.ceil((object.shape[-1]/2) - (probe.shape[-1]/2)) - 1 # For Matlab - Python index shift\n",
    "sub_px_shift = crop_coordinates - np.round(crop_coordinates) # This shift (tH, tW) would be added to the probe to compensate the integer obj cropping\n",
    "crop_indices = np.round(crop_coordinates) # This one is rounded and \n",
    "\n",
    "## Calculate propagator for multislice forward model\n",
    "extent = dx_spec * np.array(probe.shape[-2:])\n",
    "_, H, _, _ = near_field_evolution(probe.shape[-2:], z_distance, lambd, extent, use_ASM_only=True, use_np_or_cp='np')\n",
    "\n",
    "# Specify forward model accuracy options\n",
    "N_max = 7134\n",
    "pmode_max = 1 # 2\n",
    "omode_max = 1 # 64 # By default we only do 1 object mode\n",
    "\n",
    "object_data = np.broadcast_to(object, (omode_max, *object.shape))\n",
    "probe_data = probe[:pmode_max, :, :].astype('complex64')\n",
    "crop_indices_data = crop_indices[:N_max].astype('int16')\n",
    "shift_vec_data = sub_px_shift.astype('float32')\n",
    "cbeds_data = 1e8 * cbeds[:N_max].astype('float32')\n",
    "H = H.astype('complex64')\n",
    "      \n",
    "print(f\"\\nobject_data dtype/shape (omode, Nz, Ny, Nx) = {object_data.dtype}, {object_data.shape}, \\\n",
    "        \\nprobe_data dtype/shape      (pmode, Ny, Nx) = {probe_data.dtype}, {probe_data.shape}, \\\n",
    "        \\ncrop_indices_data                     (N,2) = {crop_indices_data.dtype}, {crop_indices_data.shape}, \\\n",
    "        \\nshift_vec_data                        (N,2) = {shift_vec_data.dtype}, {shift_vec_data.shape}, \\\n",
    "        \\ncbeds_data dtype/shape          (N, Ky, Kx) = {cbeds_data.dtype}, {cbeds_data.shape}, \\\n",
    "        \\nH dtype/shape                      (Ky, Kx) = {H.dtype}, {H.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Everything is still np array (complex) from this cell, it'll be converted to tensor later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Initializing optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of the modules are modified, use this to reload\n",
    "reload(models)\n",
    "reload(optimization)\n",
    "reload(visualization)\n",
    "\n",
    "from models import PtychoAD\n",
    "from optimization import CombinedLoss\n",
    "from visualization import plot_forward_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Defining PtychoAD class for the optimization object\n",
    "\n",
    "# from forward import  multislice_forward_model_vec_all\n",
    "# from utils import imshift_batch\n",
    "# import torch\n",
    "\n",
    "# # This is a current working version (2024.03.23) of the PtychoAD class\n",
    "# # I cleaned up the archived versiosn and slightly renamed the objects and variables for clarity\n",
    "\n",
    "# # set_optimizer function is called at the end of the initializaiton, while this can also be called if you want to update the optimizer params without initializing the object\n",
    "# # obj optimization is now split into objp and obja\n",
    "# # mixed object modes are normalized by the init_omode_occu. By design this is a fixed value because optimizing omode_occu with obj simultaneously could be unstable\n",
    "# # obj_ROI cropping is done with vectorization and the obj_ROI_grid is only generated once\n",
    "# # probe with sub-px shifts are calculated only when probe_pos_shifts are enables for optimization\n",
    "# # All the sub-px shifted probes in a batch are processed together with vectorizaiton\n",
    "# # Likewise, the multislice forward model is also fully vectorized across samples (in batch), pmode, and omode\n",
    "# # Note that it's possible to reduce the peak-memory consumption by reducing the level of vectorizaiton and roll back to for loops\n",
    "# # Lastly, the forward pass of this model would output the dp_fwd (N, Ky, Kx) and objp_patches (N, omode, Nz, Ny, Nx) in float32 for later loss calculation\n",
    "\n",
    "# class PtychoAD(torch.nn.Module):\n",
    "#     \"\"\" Main optimization class for the ptycho reconstruction using AD \"\"\"\n",
    "#     # Including initialization, get_obj_ROI, get_probes, and forward methods\n",
    "    \n",
    "#     # Example usage:\n",
    "#     # model = PtychoAD(init_obj, init_omode_occu, init_probe, init_crop_pos, init_probe_pos_shifts, H, \n",
    "#     #                  lr_params={'obja': 0,\n",
    "#     #                             'objp': 3e-4,\n",
    "#     #                             'probe': 1e-5, \n",
    "#     #                             'probe_pos_shifts': 0},\n",
    "#     #                  device=DEVICE)\n",
    "#     # optimizer = torch.optim.Adam(model.optimizer_params)\n",
    "\n",
    "#     def __init__(self, init_obj, init_omode_occu, init_probe, init_crop_pos, init_probe_pos_shifts, H, lr_params=None, device='cuda:0'):\n",
    "#         super(PtychoAD, self).__init__()\n",
    "#         with torch.no_grad():\n",
    "#             self.device = device\n",
    "#             self.opt_obja  = torch.abs(torch.tensor(init_obj, dtype=torch.complex64, device=device))\n",
    "#             self.opt_objp  = torch.angle(torch.tensor(init_obj, dtype=torch.complex64, device=device))\n",
    "#             self.opt_probe = torch.tensor(init_probe, dtype=torch.complex64, device=device)  \n",
    "#             self.opt_probe_pos_shifts = torch.tensor(init_probe_pos_shifts, device=device)\n",
    "#             self.omode_occu = torch.tensor(init_omode_occu, dtype=torch.float32, device=device) \n",
    "#             self.H = torch.tensor(H, dtype=torch.complex64, device=device)\n",
    "#             self.shift_probes = (lr_params['probe_pos_shifts'] != 0) # Set shift_probes to False if lr_params['probe_pos_shifts'] = 0\n",
    "            \n",
    "#             Ny, Nx = init_probe.shape[-2:]\n",
    "#             ry, rx = torch.meshgrid(torch.arange(Ny, dtype=torch.int32, device=device), torch.arange(Nx, dtype=torch.int32, device=device), indexing='ij')\n",
    "#             self.shift_probes_grid = torch.stack([ry/Ny, rx/Nx], dim=0)\n",
    "#             # Create the grid for obj_ROI in a vectorized approach\n",
    "#             # ry is the y-grid (Ny,Nx), by adding the y coordinates from init_crop_pos (N,1) in a broadcast way, it becomes (N,Ny,Nx)\n",
    "#             # Stacking the modified ry and rx at the last dimension, we get obj_ROI_grid = (N,Ny,Nx,2)\n",
    "#             self.obj_ROI_grid = torch.stack([ry[None,:,:] + torch.tensor(init_crop_pos[:, None, None, 0], device=device), \n",
    "#                                              rx[None,:,:] + torch.tensor(init_crop_pos[:, None, None, 1], device=device)], dim=-1)\n",
    "#             # Create a dictionary to store the optimizable tensors\n",
    "#             self.optimizable_tensors = {\n",
    "#                 'obja'            : self.opt_obja,\n",
    "#                 'objp'            : self.opt_objp,\n",
    "#                 'probe'           : self.opt_probe,\n",
    "#                 'probe_pos_shifts': self.opt_probe_pos_shifts}\n",
    "#             self.set_optimizer(lr_params)\n",
    "    \n",
    "#     def set_optimizer(self, lr_params):\n",
    "#         self.optimizer_params = []\n",
    "#         if lr_params:\n",
    "#             for param_name, lr in lr_params.items():\n",
    "#                 if param_name in self.optimizable_tensors:\n",
    "#                     self.optimizable_tensors[param_name].requires_grad = (lr != 0) # Set requires_grad based on learning rate\n",
    "#                     if lr != 0:\n",
    "#                         self.optimizer_params.append({'params': [self.optimizable_tensors[param_name]], 'lr': lr})\n",
    "#                 else:\n",
    "#                     print(f\"Warning: '{param_name}' is not a valid parameter name.\")\n",
    "#         # Declaring it as a ParameterDict so that I can use model.state_dict()\n",
    "#         # Note that when I wrap the former dict directly with ParameterDict it disables their grad_fn for unknown reason\n",
    "#         self.nn_params = torch.nn.ParameterDict(self.optimizable_tensors)\n",
    "        \n",
    "#         print('PtychoAD optimizable variables:')\n",
    "#         for name, tensor in self.optimizable_tensors.items():\n",
    "#             print(f\"{name.ljust(16)}: {str(tensor.shape).ljust(32)}, {str(tensor.dtype).ljust(16)}, device:{tensor.device}, grad:{str(tensor.requires_grad).ljust(5)}, lr:{lr_params[name]:.0e}\")\n",
    "#         print('\\nMake sure to pass the optimizer_params to optimizer object using \"optimizer = torch.optim.Adam(model.optimizer_params)\"')\n",
    "    \n",
    "#     def get_obj_ROI(self, indices):\n",
    "#         \"\"\" Get object ROI with integer coordinates \"\"\"\n",
    "#         # It's strongly recommended to do integer version of get_obj_ROI\n",
    "#         # opt_obj.shape = (B,D,H,W,C) = (omode,D,H,W,2)\n",
    "#         # object_patches = (N,B,D,H,W,2), N is the additional sample index within the input batch, B is now used for omode.\n",
    "        \n",
    "#         opt_obj = torch.stack([self.opt_obja, self.opt_objp], dim=-1)\n",
    "#         object_patches = opt_obj[:,:,self.obj_ROI_grid[indices,...,0],self.obj_ROI_grid[indices,...,1],:].permute(2,0,1,3,4,5)\n",
    "#         return object_patches\n",
    "    \n",
    "#     def get_probes(self, indices):\n",
    "#         \"\"\" Get probes for each position \"\"\"\n",
    "#         # If you're not trying to optimize probe positions, there's not much point using sub-px shifted stationary probes\n",
    "#         # This function will return a single probe when self.shift_probes = False,\n",
    "#         # and would only be returning multiple sub-px shifted probes if you're optimizing self.opt_probe_pos_shifts\n",
    "\n",
    "#         if self.shift_probes:\n",
    "#             probes = imshift_batch(self.opt_probe, shifts = self.opt_probe_pos_shifts[indices], grid = self.shift_probes_grid)\n",
    "#         else:\n",
    "#             probes = self.opt_probe[None,...] # Extend a singleton N dimension, essentially using same probe for all samples\n",
    "#         return probes\n",
    "        \n",
    "#     def forward(self, indices):\n",
    "#         \"\"\" Doing the forward pass and get an output diffraction pattern for each input index \"\"\"\n",
    "#         # The indices are passed in as an array and representing the whole batch\n",
    "        \n",
    "#         object_patches = self.get_obj_ROI(indices)\n",
    "#         probes = self.get_probes(indices)\n",
    "#         dp_fwd = multislice_forward_model_vec_all(object_patches, self.omode_occu, probes, self.H)\n",
    "#         return dp_fwd, object_patches[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init_obj              = np.exp(1j * 0.05*np.random.rand(1,1,391,403)).astype('complex64')\n",
    "init_obj              = np.exp(1j * 0.05*np.random.rand(*object_data.shape)).astype('complex64')\n",
    "#init_obj              = object_data\n",
    "#init_omode_occu       = [0.92, 0.02, 0.02, 0.02, 0.02] #np.ones(len(init_obj))/len(init_obj)\n",
    "init_omode_occu       = np.ones(len(init_obj))/len(init_obj)\n",
    "init_probe            = probe_data\n",
    "init_crop_pos         = crop_indices_data\n",
    "init_probe_pos_shifts = shift_vec_data\n",
    "measurements  = torch.from_numpy(cbeds_data).to(DEVICE)\n",
    "\n",
    "model = PtychoAD(init_obj, init_omode_occu, init_probe, init_crop_pos, init_probe_pos_shifts, H, \n",
    "                lr_params={'obja': 0,\n",
    "                           'objp': 3e-4,\n",
    "                           'probe': 1e-5, \n",
    "                           'probe_pos_shifts': 0},\n",
    "                device=DEVICE)\n",
    "\n",
    "# Use model.set_optimizer(new_lr_params) to update the variable flag and optimizer_params\n",
    "optimizer = torch.optim.Adam(model.optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the MS-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Roll the last 2 z-slice and reverse the order\n",
    "# modified_tensor = torch.concatenate([model.opt_objp[:,-3:].flip(1), model.opt_objp[:,:-3]], dim=1)\n",
    "# modified_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,5))\n",
    "# plt.imshow(modified_tensor[0,:,195,:].detach().cpu().numpy())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.opt_objp.data = modified_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.01 Check the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0,N_max,3)\n",
    "dp_power = 0.5\n",
    "\n",
    "plot_forward_pass(model, indices, dp_power, object_data, crop_indices_data, cbeds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Main optimization loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the loss params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_params = {\n",
    "    'loss_single': {'state':  True,  'weight': 1.0, 'dp_pow': 0.5},\n",
    "    'loss_pacbed': {'state': False,  'weight': 1.0, 'dp_pow': 0.2},\n",
    "    'loss_tv'    : {'state': False,  'weight': 1e-4},\n",
    "    'loss_l1'    : {'state': False,  'weight': 1.0},\n",
    "    'loss_l2'    : {'state': False,  'weight': 1.0}\n",
    "}\n",
    "with torch.no_grad():\n",
    "    loss_fn = CombinedLoss(loss_params, device=DEVICE)\n",
    "    #np.random.seed(42)\n",
    "    indices = np.random.randint(0,N_max,40)\n",
    "    model_CBEDs, objp_patches = model(indices)\n",
    "    measured_CBEDs = measurements[indices]\n",
    "    loss_batch, losses = loss_fn(model_CBEDs, measured_CBEDs, objp_patches, model.omode_occu)\n",
    "\n",
    "# Print loss_name and loss_value with padding\n",
    "for loss_name, loss_value in zip(loss_params.keys(), losses):\n",
    "    print(f\"{loss_name.ljust(11)}: {loss_value.detach().cpu().numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = 'output/CNS_1obj_10slice_3e-4_random_clamp/'\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "NITER = 10\n",
    "BATCH_SIZE = 256\n",
    "num_batch = len(measurements) / BATCH_SIZE # The actual batch size would only be \"close\" if it's not divisible by len(measurments)\n",
    "cbed_shape = measurements.shape[1:]\n",
    "loss_iters = []\n",
    "\n",
    "\n",
    "for iter in range(NITER+1):\n",
    "    batch_losses = {name: [] for name in loss_params.keys()}\n",
    "    shuffled_indices = np.random.choice(len(measurements), size=len(measurements), replace=False) # Creates a shuffled 1D array of indices\n",
    "    batches = np.array_split(shuffled_indices, num_batch)                                         # return a list of `num_batch` arrays, or [batch0, batch1, ...]\n",
    "    start_iter_t = time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        start_batch_t = time()\n",
    "        optimizer.zero_grad()\n",
    "        model_CBEDs, objp_patches = model(batch)\n",
    "        measured_CBEDs = measurements[batch]\n",
    "        loss_batch, losses = loss_fn(model_CBEDs, measured_CBEDs, objp_patches, model.omode_occu)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        end_batch_t = time()\n",
    "\n",
    "        for loss_name, loss_value in zip(loss_params.keys(), losses):\n",
    "            batch_losses[loss_name].append(loss_value.detach().cpu().numpy())\n",
    "\n",
    "        if batch_idx in np.linspace(0, len(batches) - 1, num=6, dtype=int):\n",
    "            print(f\"Done batch {batch_idx} in iter {iter} in {(end_batch_t - start_batch_t):.3f} sec\")\n",
    "            \n",
    "    torch.cuda.synchronize()\n",
    "    end_iter_t = time()\n",
    "\n",
    "    avg_losses = {name: np.mean(values) for name, values in batch_losses.items()}\n",
    "    loss_str = ', '.join([f\"{name}: {value:.4f}\" for name, value in avg_losses.items()])\n",
    "    print(f\"Iter: {iter}, Total Loss: {sum(avg_losses.values()):.4f}, {loss_str}, \"\n",
    "          f\"in {(end_iter_t - start_iter_t) // 60} min {(end_iter_t - start_iter_t) % 60:03f} sec\")\n",
    "\n",
    "    loss_iters.append(sum(avg_losses.values()))\n",
    "    \n",
    "    # Do a hard positivity constraint at the end of each iter without grad\n",
    "    with torch.no_grad():\n",
    "        print(f\"Applying hard positivity constraint to obj phase after iter {iter}\")\n",
    "        model.opt_objp.data.clamp_(min=0) # Doing .clamp_ is similar to passing a softplus\n",
    "        #model.opt_objp.data = softplus_fn(model.opt_objp)\n",
    "        \n",
    "    \n",
    "    # # # ## Saving\n",
    "    # if iter % 50 == 0:\n",
    "    #     torch.save(model.state_dict(), os.path.join(output_path, f\"model_CNS_64obj_3e-4_iter{str(iter).zfill(4)}.pt\"))\n",
    "    #     imwrite(os.path.join(output_path, f\"image_CNS_64obj_3e-4_iter{str(iter).zfill(4)}.tif\"), model.opt_objp.detach().cpu().numpy()[:,0].astype('float32'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imwrite(os.path.join(output_path, f\"image_CNS_1obj_3e-4_iter300_loss1.67_sqsqrt.tif\"), model.opt_objp.detach().cpu().numpy()[0,0].astype('float32'))\n",
    "imwrite(os.path.join(output_path, f\"image_CNS_1obj_10slice_3e-4_iter100_clamp_roll5.tif\"), model.opt_objp.detach().cpu().numpy().astype('float32'))\n",
    "torch.save(model.state_dict(), os.path.join(output_path, f\"model_CNS_1obj_10slice_3e-4_iter100_clamp_roll5.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0,N_max,3)\n",
    "dp_power = 0.2\n",
    "\n",
    "plot_forward_pass(model, indices, dp_power, object_data, crop_indices_data, cbeds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(8,8))\n",
    "im00=axs[0,0].imshow(model.opt_probe[0].abs().detach().cpu())\n",
    "im01=axs[0,1].imshow(np.abs(probe_data[0]))\n",
    "im10=axs[1,0].imshow(model.opt_probe[0].angle().detach().cpu())\n",
    "im11=axs[1,1].imshow(np.angle(probe_data[0]))\n",
    "\n",
    "axs[0,0].set_title(\"Model probe (amp)\")\n",
    "axs[0,1].set_title(\"Data probe (amp)\")\n",
    "axs[1,0].set_title(\"Model probe (phase)\")\n",
    "axs[1,1].set_title(\"Data probe (phase)\")\n",
    "\n",
    "\n",
    "fig.colorbar(im00)\n",
    "fig.colorbar(im01)\n",
    "fig.colorbar(im10)\n",
    "fig.colorbar(im11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([x for x in loss_iters][220:])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptyrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
