/home/fs01/cl2696/workspace/ptyrad
c0001
Mon Sep 23 23:30:48 EDT 2024
params_path = params/demo/tBL_WSe2_reconstruct.yml
Mon Sep 23 23:30:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   35C    P0    59W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
PyTorch version:  2.4.1
CUDA available:  True
CUDA version:  11.8
CUDA device count:  1
CUDA device:  ['NVIDIA A100-SXM4-80GB']
ptyrad version: v0.1.0-beta2.5
### Loading params file ###
Success! Loaded .yml file path = params/demo/tBL_WSe2_reconstruct.yml
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Accelerator DataLoader split_batches = True

### Initializing Initializer ###

### Initializing cache ###
use_cached_obj   = False
use_cached_probe = False
use_cached_pos   = False

### Initializing exp_params ###
Input values are displayed below:
kv: 80
conv_angle: 24.9
Npix: 128
dx_spec: 0.1494
defocus: 0
c3: 0
z_distance: 1
Nlayer: 12
N_scans: 16384
N_scan_slow: 128
N_scan_fast: 128
scan_step_size: 0.429
scan_flipT: None
scan_affine: None
scan_rand_std: 0.15
omode_max: 1
omode_init_occu: {'occu_type': 'uniform', 'init_occu': None}
pmode_max: 6
pmode_init_pows: [0.02]
probe_permute: None
meas_permute: None
meas_reshape: None
meas_flipT: [1, 0, 0]
meas_crop: None
meas_resample: None
meas_add_source_size: None
meas_add_detector_blur: None
meas_add_poisson_noise: None

Derived values given input exp_params:
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7495 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0230 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang

### Initializing measurements from 'raw' ###
Imported meausrements shape = (16384, 128, 128)
Imported meausrements int. statistics (min, mean, max) = (-30.8785, 1814.7064, 99441.8047)
Flipping measurements with [flipup, fliplr, transpose] = [1, 0, 0]
Minimum value of -30.8785 subtracted due to the positive px value constraint of measurements
Normalizing measurements so the averaged measurement has max intensity at 1
Radius of bright field disk             (rbf) = 11.0 px, suggested probe_mask_k radius (rbf*2/Npix) > 0.17
meausrements int. statistics (min, mean, max) = (0.0000, 0.0275, 1.4818)
measurements                      (N, Ky, Kx) = float32, (16384, 128, 128)

### Initializing probe from 'simu' ###
Use exp_params and default values instead for simulation
Start simulating STEM probe
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7447 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0229 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang
Start making mixed-state STEM probe with 6 incoherent probe modes
Relative power of probe modes = [0.9  0.02 0.02 0.02 0.02 0.02]
probe                         (pmode, Ny, Nx) = complex64, (6, 128, 128)
sum(|probe_data|**2) = 450.46, while sum(meas)/len(meas) = 450.46

### Initializing probe pos from 'simu' ###
Simulating probe positions with dx_spec = 0.1494, scan_step_size = 0.429, N_scan_fast = 128, N_scan_slow = 128
Applying Gaussian distributed random displacement with std = 0.15 px to scan positions
crop_pos                                (N,2) = int16, (16384, 2)
crop_pos 1st and last px coords (y,x)         = ([50, 50], [414, 414])
crop_pos extent (Ang)                         = [54.6804 54.6804]
probe_pos_shifts                        (N,2) = float32, (16384, 2)

### Initializing obj from 'simu' ###
obj_shape is not provided, use exp_params, position range, and probe shape for estimated obj_shape (omode, Nz, Ny, Nx)
object                    (omode, Nz, Ny, Nx) = complex64, (1, 12, 592, 592)
object extent                 (Z, Y, X) (Ang) = [12.     88.4448 88.4448]

### Initializing omode_occu from 'uniform' ###
omode_occu                            (omode) = float32, (1,)

### Initializing H (Fresnel propagator) ###
Calculating H with probe_shape = [128 128], z_distance = 1.0000 Ang, lambd = 0.0418 Ang, extent = [19.1232 19.1232] Ang
H                                    (Ky, Kx) = complex64, (128, 128)

### Initializing obj tilts from = 'simu' ###
Initialized obj_tilts with init_tilts = [[0, 0]] (theta_y, theta_x) mrad
obj_tilts                              (N, 2) = float32, (1, 2)

### Checking consistency between input params with the initialized variables ###
Npix, DP measurements, probe, and H shapes are consistent as '128'
N_scans, len(meas), N_scan_slow*N_scan_fast, len(crop_pos), and len(probe_pos_shifts) are consistent as '16384'
obj.shape[0] is consistent with len(omode_occu) as '1'
obj.shape[1] is consistent with Nlayer as '12'
obj_tilts is consistent with either 1 or N_scans
Pass the consistency check of initialized variables, initialization is done!

### Initializing loss function ###

### Initializing constraint function ###

### Done initializing PtyRADSolver ###

### Starting the PtyRADSolver in reconstruct mode ###

### PtychoAD optimizable variables ###
obja            : torch.Size([1, 12, 592, 592])   , torch.float32   , device:cuda:0, grad:True , lr:5e-03
objp            : torch.Size([1, 12, 592, 592])   , torch.float32   , device:cuda:0, grad:True , lr:5e-03
obj_tilts       : torch.Size([1, 2])              , torch.float32   , device:cuda:0, grad:False, lr:0e+00
probe           : torch.Size([6, 128, 128, 2])    , torch.float32   , device:cuda:0, grad:True , lr:1e-03
probe_pos_shifts: torch.Size([16384, 2])          , torch.float32   , device:cuda:0, grad:True , lr:5e-03

### Optimizable variables statitsics ###
Total measurement values:    268,435,456                
Total optimizing variables:  8,640,512                
Overdetermined ratio:        31.07

### Model behavior ###
Obj preblur       : False
Tilt propagator   : False
Sub-px probe shift: True
Detector blur     : False

### Creating PyTorch 'Adam' optimizer with configs = {} ###

### Generating indices, batches, and output_path ###
d90 = 23.000 px or 3.436 Ang
Selecting indices with the 'full' mode 
Generated 4 'random' groups of ~4096 scan positions in 0.001 sec
The effective batch size (i.e., how many probe positions are simultaneously used for 1 update of ptychographic parameters) is batch_size * grad_accumulation = 4096 * 1 = 4096
output_path = 'output/tBL-WSe2//20240923_full_N16384_dp128_flipT100_random4096_p6_1obj_12slice_dz1_Adam_plr1e-3_oalr5e-3_oplr5e-3_slr5e-3_ozblur1_opos_sng1.0_spr0.1_altas_benchmark_normal' is generated!
Successfully copy 'tBL_WSe2_reconstruct.yml' to 'output/tBL-WSe2//20240923_full_N16384_dp128_flipT100_random4096_p6_1obj_12slice_dz1_Adam_plr1e-3_oalr5e-3_oplr5e-3_slr5e-3_ozblur1_opos_sng1.0_spr0.1_altas_benchmark_normal'
len(DataLoader) = num_batches = 4, DataLoader.batch_size = 4096

### Start the PtyRAD iterative ptycho reconstruction ###
Iter: 1, obja.requires_grad = True
Iter: 1, objp.requires_grad = True
Iter: 1, obj_tilts.requires_grad = False
Iter: 1, probe.requires_grad = True
Iter: 1, probe_pos_shifts.requires_grad = True
Done batch 1 with 4096 indices ([5977, 8620, 12383, 10916, 41]...) in 2.928 sec
Traceback (most recent call last):
  File "/home/fs01/cl2696/workspace/ptyrad/./scripts/run_ptyrad.py", line 39, in <module>
    ptycho_solver.run()
  File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 173, in run
    self.reconstruct()
  File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 119, in reconstruct
    recon_loop(model, self.init, params, optimizer, self.loss_fn, self.constraint_fn, indices, self.dl, output_path, acc=self.accelerator)
  File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 307, in recon_loop
    batch_losses, iter_t = recon_step(batches, grad_accumulation, model, optimizer, loss_fn, constraint_fn, niter, verbose=verbose, acc=acc)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 380, in recon_step
    acc.backward(loss_batch)
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/accelerator.py", line 2196, in backward
    loss.backward(**kwargs)
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 79.20 GiB of which 3.58 GiB is free. Including non-PyTorch memory, this process has 75.61 GiB memory in use. Of the allocated memory 62.60 GiB is allocated by PyTorch, and 11.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Mon Sep 23 23:31:08 EDT 2024
