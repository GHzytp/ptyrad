/home/fs01/cl2696/workspace/ptyrad
c0001
Mon Sep 23 23:13:36 EDT 2024
params_path = params/demo/tBL_WSe2_reconstruct.yml
Mon Sep 23 23:13:37 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   34C    P0    62W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   34C    P0    59W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
PyTorch version:  2.4.1
PyTorch version:  2.4.1
CUDA available:  True
CUDA version:  11.8
CUDA available:  True
CUDA version:  11.8
CUDA device count:  2
CUDA device count:  2
CUDA device:  ['NVIDIA A100-SXM4-80GB', 'NVIDIA A100-SXM4-80GB']
ptyrad version: v0.1.0-beta2.5
CUDA device:  ['NVIDIA A100-SXM4-80GB', 'NVIDIA A100-SXM4-80GB']
ptyrad version: v0.1.0-beta2.5
### Loading params file ###### Loading params file ###

Success! Loaded .yml file path = params/demo/tBL_WSe2_reconstruct.yml
Success! Loaded .yml file path = params/demo/tBL_WSe2_reconstruct.yml
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Accelerator DataLoader split_batches = True

### Initializing Initializer ###

### Initializing cache ###
use_cached_obj   = False
use_cached_probe = False
use_cached_pos   = False

### Initializing exp_params ###
Input values are displayed below:
kv: 80
conv_angle: 24.9
Npix: 128
dx_spec: 0.1494
defocus: 0
c3: 0
z_distance: 1
Nlayer: 12
N_scans: 16384
N_scan_slow: 128
N_scan_fast: 128
scan_step_size: 0.429
scan_flipT: None
scan_affine: None
scan_rand_std: 0.15
omode_max: 1
omode_init_occu: {'occu_type': 'uniform', 'init_occu': None}
pmode_max: 6
pmode_init_pows: [0.02]
probe_permute: None
meas_permute: None
meas_reshape: None
meas_flipT: [1, 0, 0]
meas_crop: None
meas_resample: None
meas_add_source_size: None
meas_add_detector_blur: None
meas_add_poisson_noise: None

Derived values given input exp_params:
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7495 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0230 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang

### Initializing measurements from 'raw' ###

Derived values given input exp_params:
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7495 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0230 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang
Imported meausrements shape = (16384, 128, 128)
Imported meausrements int. statistics (min, mean, max) = (-30.8785, 1814.7064, 99441.8047)
Flipping measurements with [flipup, fliplr, transpose] = [1, 0, 0]
Minimum value of -30.8785 subtracted due to the positive px value constraint of measurements
Normalizing measurements so the averaged measurement has max intensity at 1
Radius of bright field disk             (rbf) = 11.0 px, suggested probe_mask_k radius (rbf*2/Npix) > 0.17
meausrements int. statistics (min, mean, max) = (0.0000, 0.0275, 1.4818)
measurements                      (N, Ky, Kx) = float32, (16384, 128, 128)

### Initializing probe from 'simu' ###
Use exp_params and default values instead for simulation
Start simulating STEM probe
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7447 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0229 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang
Start making mixed-state STEM probe with 6 incoherent probe modes
Relative power of probe modes = [0.9  0.02 0.02 0.02 0.02 0.02]
kv          = 80 kV
wavelength  = 0.0418 Ang
conv_angle  = 24.9 mrad
Npix        = 128 px
dk          = 0.0523 Ang^-1
kMax        = 3.3467 Ang^-1
alpha_max   = 139.7447 mrad
dx          = 0.1494 Ang, Nyquist-limited dmin = 2*dx = 0.2988 Ang
Rayleigh-limited resolution  = 1.0229 Ang (0.61*lambda/alpha for focused probe )
Real space probe extent = 19.1232 Ang
probe                         (pmode, Ny, Nx) = complex64, (6, 128, 128)
sum(|probe_data|**2) = 450.46, while sum(meas)/len(meas) = 450.46

### Initializing probe pos from 'simu' ###
Simulating probe positions with dx_spec = 0.1494, scan_step_size = 0.429, N_scan_fast = 128, N_scan_slow = 128
Applying Gaussian distributed random displacement with std = 0.15 px to scan positions
crop_pos                                (N,2) = int16, (16384, 2)
crop_pos 1st and last px coords (y,x)         = ([50, 50], [414, 414])
crop_pos extent (Ang)                         = [54.6804 54.6804]
probe_pos_shifts                        (N,2) = float32, (16384, 2)

### Initializing obj from 'simu' ###
obj_shape is not provided, use exp_params, position range, and probe shape for estimated obj_shape (omode, Nz, Ny, Nx)
object                    (omode, Nz, Ny, Nx) = complex64, (1, 12, 592, 592)
object extent                 (Z, Y, X) (Ang) = [12.     88.4448 88.4448]

### Initializing omode_occu from 'uniform' ###
omode_occu                            (omode) = float32, (1,)

### Initializing H (Fresnel propagator) ###
Calculating H with probe_shape = [128 128], z_distance = 1.0000 Ang, lambd = 0.0418 Ang, extent = [19.1232 19.1232] Ang
H                                    (Ky, Kx) = complex64, (128, 128)

### Initializing obj tilts from = 'simu' ###
Initialized obj_tilts with init_tilts = [[0, 0]] (theta_y, theta_x) mrad
obj_tilts                              (N, 2) = float32, (1, 2)

### Checking consistency between input params with the initialized variables ###
Npix, DP measurements, probe, and H shapes are consistent as '128'
N_scans, len(meas), N_scan_slow*N_scan_fast, len(crop_pos), and len(probe_pos_shifts) are consistent as '16384'
obj.shape[0] is consistent with len(omode_occu) as '1'
obj.shape[1] is consistent with Nlayer as '12'
obj_tilts is consistent with either 1 or N_scans
Pass the consistency check of initialized variables, initialization is done!

### Initializing loss function ###

### Initializing constraint function ###

### Done initializing PtyRADSolver ###

### Starting the PtyRADSolver in reconstruct mode ###

### PtychoAD optimizable variables ###
obja            : torch.Size([1, 12, 592, 592])   , torch.float32   , device:cuda:0, grad:True , lr:5e-03
objp            : torch.Size([1, 12, 592, 592])   , torch.float32   , device:cuda:0, grad:True , lr:5e-03
obj_tilts       : torch.Size([1, 2])              , torch.float32   , device:cuda:0, grad:False, lr:0e+00
probe           : torch.Size([6, 128, 128, 2])    , torch.float32   , device:cuda:0, grad:True , lr:1e-03
probe_pos_shifts: torch.Size([16384, 2])          , torch.float32   , device:cuda:0, grad:True , lr:5e-03

### Optimizable variables statitsics ###
Total measurement values:    268,435,456                
Total optimizing variables:  8,640,512                
Overdetermined ratio:        31.07

### Model behavior ###
Obj preblur       : False
Tilt propagator   : False
Sub-px probe shift: True
Detector blur     : False

### Creating PyTorch 'Adam' optimizer with configs = {} ###

### Generating indices, batches, and output_path ###
d90 = 23.000 px or 3.436 Ang
Selecting indices with the 'full' mode 
Generated 2 'random' groups of ~8192 scan positions in 0.002 sec
d90 = 23.000 px or 3.436 Ang
The effective batch size (i.e., how many probe positions are simultaneously used for 1 update of ptychographic parameters) is batch_size * grad_accumulation = 8192 * 1 = 8192
output_path = 'output/tBL-WSe2//20240923_full_N16384_dp128_flipT100_random8192_p6_1obj_12slice_dz1_Adam_plr1e-3_oalr5e-3_oplr5e-3_slr5e-3_ozblur1_opos_sng1.0_spr0.1_altas_benchmark_acc2' is generated!
Successfully copy 'tBL_WSe2_reconstruct.yml' to 'output/tBL-WSe2//20240923_full_N16384_dp128_flipT100_random8192_p6_1obj_12slice_dz1_Adam_plr1e-3_oalr5e-3_oplr5e-3_slr5e-3_ozblur1_opos_sng1.0_spr0.1_altas_benchmark_acc2'
len(DataLoader) = num_batches = 2, DataLoader.batch_size = 8192
len(DataLoader) = num_batches = 2, DataLoader.batch_size = 8192

### Start the PtyRAD iterative ptycho reconstruction ###
Iter: 1, obja.requires_grad = True
Iter: 1, objp.requires_grad = True
Iter: 1, obj_tilts.requires_grad = False
Iter: 1, probe.requires_grad = True
Iter: 1, probe_pos_shifts.requires_grad = True
Done batch 1 with 4096 indices ([11638, 12678, 4201, 8530, 15853]...) in 3.151 sec
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/fs01/cl2696/workspace/ptyrad/./scripts/run_ptyrad.py", line 39, in <module>
[rank0]:     ptycho_solver.run()
[rank0]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 173, in run
[rank0]:     self.reconstruct()
[rank0]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 119, in reconstruct
[rank0]:     recon_loop(model, self.init, params, optimizer, self.loss_fn, self.constraint_fn, indices, self.dl, output_path, acc=self.accelerator)
[rank0]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 307, in recon_loop
[rank0]:     batch_losses, iter_t = recon_step(batches, grad_accumulation, model, optimizer, loss_fn, constraint_fn, niter, verbose=verbose, acc=acc)
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 380, in recon_step
[rank0]:     acc.backward(loss_batch)
[rank0]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/accelerator.py", line 2196, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 79.20 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 75.86 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/fs01/cl2696/workspace/ptyrad/./scripts/run_ptyrad.py", line 39, in <module>
[rank1]:     ptycho_solver.run()
[rank1]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 173, in run
[rank1]:     self.reconstruct()
[rank1]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 119, in reconstruct
[rank1]:     recon_loop(model, self.init, params, optimizer, self.loss_fn, self.constraint_fn, indices, self.dl, output_path, acc=self.accelerator)
[rank1]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 307, in recon_loop
[rank1]:     batch_losses, iter_t = recon_step(batches, grad_accumulation, model, optimizer, loss_fn, constraint_fn, niter, verbose=verbose, acc=acc)
[rank1]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/fs01/cl2696/workspace/ptyrad/ptyrad/reconstruction.py", line 380, in recon_step
[rank1]:     acc.backward(loss_batch)
[rank1]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/accelerator.py", line 2196, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/_tensor.py", line 521, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 1 has a total capacity of 79.20 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 75.86 GiB memory in use. Of the allocated memory 62.63 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W923 23:14:06.862806496 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0923 23:14:08.644000 23210401485824 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 421686 closing signal SIGTERM
E0923 23:14:09.460000 23210401485824 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 421685) of binary: /home/fs01/cl2696/anaconda3/envs/ptyrad_acc/bin/python3.12
Traceback (most recent call last):
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fs01/cl2696/anaconda3/envs/ptyrad_acc/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./scripts/run_ptyrad.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-23_23:14:08
  host      : c0001.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 421685)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Mon Sep 23 23:14:10 EDT 2024
